{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    2\n",
      "4    1\n",
      "Name: country_destination, dtype: int64\n",
      "   timestamp_first_active    gender       age  signup_method  signup_flow  \\\n",
      "0               -4.380020 -0.927300 -0.163283      -1.596552    -0.427798   \n",
      "1               -4.357961  1.058047  0.287705      -1.596552    -0.427798   \n",
      "2               -4.348661 -0.927300  2.317149       0.628333    -0.035009   \n",
      "3               -4.303076 -0.927300  0.738692      -1.596552    -0.427798   \n",
      "4               -4.283949 -0.927300  0.625945       0.628333    -0.427798   \n",
      "\n",
      "   language  affiliate_channel  affiliate_provider  first_affiliate_tracked  \\\n",
      "0 -0.141579          -0.582242           -0.468760                -0.798954   \n",
      "1 -0.141579           2.556797            0.251719                -0.798954   \n",
      "2 -0.141579          -0.582242           -0.468760                -0.798954   \n",
      "3 -0.141579          -0.582242           -0.468760                -0.798954   \n",
      "4 -0.141579          -0.582242           -0.468760                -0.798954   \n",
      "\n",
      "   signup_app      ...       Linux Desktop  Mac Desktop  Opera Phone  \\\n",
      "0   -0.359375      ...           -0.120933    -0.220553    -0.726855   \n",
      "1   -0.359375      ...           -0.120933    -0.220553    -0.726855   \n",
      "2   -0.359375      ...           -0.120933    -0.220553    -0.726855   \n",
      "3   -0.359375      ...           -0.120933    -0.220553    -0.726855   \n",
      "4   -0.359375      ...           -0.120933    -0.220553    -0.726855   \n",
      "\n",
      "     Tablet  Windows Desktop  Windows Phone  iPad Tablet    iPhone  iPodtouch  \\\n",
      "0 -0.062776        -0.196923      -0.707107    -0.108443 -0.133838  -0.153578   \n",
      "1 -0.062776        -0.196923      -0.707107    -0.108443 -0.133838  -0.153578   \n",
      "2 -0.062776        -0.196923      -0.707107    -0.108443 -0.133838  -0.153578   \n",
      "3 -0.062776        -0.196923      -0.707107    -0.108443 -0.133838  -0.153578   \n",
      "4 -0.062776        -0.196923      -0.707107    -0.108443 -0.133838  -0.153578   \n",
      "\n",
      "   secs_elapsed  \n",
      "0     -0.391996  \n",
      "1     -0.391996  \n",
      "2     -0.391996  \n",
      "3     -0.391996  \n",
      "4     -0.391996  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "Index(['timestamp_first_active', 'gender', 'age', 'signup_method',\n",
      "       'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider',\n",
      "       'first_affiliate_tracked', 'signup_app', 'first_device_type',\n",
      "       'first_browser', 'created_year', 'created_month', 'created_day',\n",
      "       'session_count', '-unknown-_x', 'booking_request', 'booking_response',\n",
      "       'click', 'data', 'message_post', 'modify', 'partner_callback', 'submit',\n",
      "       'view', '-unknown-_y', 'Android App Unknown Phone/Tablet',\n",
      "       'Android Phone', 'Blackberry', 'Chromebook', 'Linux Desktop',\n",
      "       'Mac Desktop', 'Opera Phone', 'Tablet', 'Windows Desktop',\n",
      "       'Windows Phone', 'iPad Tablet', 'iPhone', 'iPodtouch', 'secs_elapsed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Read the train and the test data \n",
    "#The preprocessing file needs to be run first, it creates the csv file used here\n",
    "train_users = pd.read_csv('train_users_merge_scale.csv')\n",
    "#test_users = pd.read_csv('test_users.csv')\n",
    "\n",
    "\n",
    "# Extracting labels from the train data\n",
    "train_users_labels = train_users.loc[:,'country_destination']\n",
    "print (train_users_labels.head(n=5))\n",
    "\n",
    "# Extracting attributes from the train data\n",
    "train_users_attrs = train_users.iloc[:,:-1]\n",
    "print(train_users_attrs.head(n=5))\n",
    "\n",
    "train_users = train_users_attrs\n",
    "print(train_users.columns)\n",
    "train_users = train_users.drop(['Blackberry'], axis=1)\n",
    "train_users = train_users.drop(['Opera Phone'], axis=1)\n",
    "labels_df = pd.DataFrame(train_users_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def dcg_score(y_true, y_score, k=5):\n",
    "\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    gain = 2 ** y_true - 1\n",
    "\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "#def ndcg_score(ground_truth, predictions, k=5):\n",
    "def ndcg_score(te_labels, predict, k):\n",
    "   \n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(range(12 + 1))\n",
    "    T = lb.transform(te_labels)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    # Iterate over each y_true and compute the DCG score\n",
    "    for y_true, y_score in zip(T, predict):\n",
    "        actual = dcg_score(y_true, y_score, k)\n",
    "        best = dcg_score(y_true, y_true, k)\n",
    "        if best == 0:\n",
    "            best = 0.000000001\n",
    "        score = float(actual) / float(best)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# NDCG Scorer function\n",
    "ndcg_scorer = make_scorer(ndcg_score, needs_proba=True, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def folds_to_split(data,targets,train,test):\n",
    "    data_tr = pd.DataFrame(data).iloc[train]\n",
    "    data_te = pd.DataFrame(data).iloc[test]\n",
    "    labels_tr = pd.DataFrame(targets).iloc[train]\n",
    "    labels_te = pd.DataFrame(targets).iloc[test]\n",
    "    return [data_tr, data_te, labels_tr, labels_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------OneVsRestClassifier accuracy and ndcg values------\n",
      "    Accuracy_newton-cg  ndcg_newton-cg  Accuracy_lbfgs  ndcg_lbfgs  \\\n",
      "1             0.607278        0.815000        0.607278    0.815000   \n",
      "2             0.603587        0.813433        0.603587    0.813436   \n",
      "3             0.604344        0.812489        0.604344    0.812489   \n",
      "4             0.602073        0.812163        0.602073    0.812145   \n",
      "5             0.604581        0.813422        0.604581    0.813422   \n",
      "6             0.613099        0.817676        0.613099    0.817676   \n",
      "7             0.610212        0.816933        0.610212    0.816914   \n",
      "8             0.610117        0.816962        0.610117    0.816962   \n",
      "9             0.608272        0.815800        0.608272    0.815800   \n",
      "10            0.608461        0.816193        0.608461    0.816191   \n",
      "\n",
      "    Accuracy_liblinear  ndcg_liblinear  \n",
      "1             0.607278        0.815000  \n",
      "2             0.603587        0.813433  \n",
      "3             0.604344        0.812489  \n",
      "4             0.602073        0.812163  \n",
      "5             0.604581        0.813422  \n",
      "6             0.613099        0.817676  \n",
      "7             0.610212        0.816933  \n",
      "8             0.610165        0.816979  \n",
      "9             0.608272        0.815819  \n",
      "10            0.608461        0.816193  \n",
      "------OneVsRestClassifier mean accuracy, ndcg values------\n",
      "Accuracy_newton-cg    0.607202\n",
      "ndcg_newton-cg        0.815007\n",
      "Accuracy_lbfgs        0.607202\n",
      "ndcg_lbfgs            0.815004\n",
      "Accuracy_liblinear    0.607207\n",
      "ndcg_liblinear        0.815011\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression : One Versus Rest\n",
    "# Use validation set to find which solver to use \n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model, cross_validation\n",
    "\n",
    "fold_results_ovr = pd.DataFrame()\n",
    "test_sets = []\n",
    "test_set_labels = []\n",
    "train_sets = []\n",
    "train_set_labels = []\n",
    "    \n",
    "def ten_fold_oneVsRest(data, labels):\n",
    "    \n",
    "    for train, test in cross_validation.KFold(len(data), n_folds=10, shuffle=True, random_state=20160202):\n",
    "        [tr_data, te_data,\n",
    "         tr_target, te_target] = folds_to_split(data, labels,train,test)\n",
    "        \n",
    "        train_sets.append(tr_data)\n",
    "        train_set_labels.append(tr_target)\n",
    "        test_sets.append(te_data)\n",
    "        test_set_labels.append(te_target)\n",
    "        \n",
    "    foldnum = 0\n",
    "    solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "    for x in range(0, 10):\n",
    "        foldnum+=1\n",
    "        [bnb_train, bnb_validation, bnb_train_labels, bnb_validation_labels] = cross_validation.train_test_split(\n",
    "        train_sets[x], \n",
    "        train_set_labels[x], \n",
    "        test_size=0.11, \n",
    "        random_state=20160121)\n",
    "        \n",
    "        for sol in solvers:\n",
    "            oneVsRest = OneVsRestClassifier(linear_model.LogisticRegression(solver=sol))\n",
    "            #print(tr_target)\n",
    "            oneVsRest.fit(bnb_train.values, bnb_train_labels[bnb_train_labels.columns.values[0]].values)\n",
    "            #print(oneVsRest.estimators_)\n",
    "        \n",
    "            fold_results_ovr.loc[foldnum, 'Accuracy_' + sol] = oneVsRest.score(bnb_validation, bnb_validation_labels)\n",
    "        \n",
    "            predictions = oneVsRest.predict_proba(bnb_validation)\n",
    "            score = ndcg_score(bnb_validation_labels.as_matrix(), predictions, 5)\n",
    "            fold_results_ovr.loc[foldnum, 'ndcg_' + sol]  = score\n",
    "            #print(score)\n",
    "                \n",
    "    #Now let's look at the results:\n",
    "    print (\"------OneVsRestClassifier accuracy and ndcg values------\")\n",
    "    print (fold_results_ovr)\n",
    "    print (\"------OneVsRestClassifier mean accuracy, ndcg values------\")\n",
    "    print(fold_results_ovr.mean())\n",
    "    \n",
    "ten_fold_oneVsRest(train_users, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------OneVsRestClassifier accuracy and ndcg values------\n",
      "    Accuracy      ndcg\n",
      "1   0.608076  0.816181\n",
      "2   0.609885  0.816683\n",
      "3   0.604685  0.813442\n",
      "4   0.606840  0.815432\n",
      "5   0.606606  0.815835\n",
      "6   0.604872  0.814452\n",
      "7   0.607121  0.813901\n",
      "8   0.602670  0.813187\n",
      "9   0.610588  0.818077\n",
      "10  0.611525  0.816938\n",
      "------OneVsRestClassifier mean accuracy, ndcg values------\n",
      "Accuracy    0.607287\n",
      "ndcg        0.815413\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "fold_results_ovr = pd.DataFrame()\n",
    "foldnum = 0\n",
    "for x in range(0, 10):\n",
    "    foldnum+=1\n",
    "    oneVsRest = OneVsRestClassifier(linear_model.LogisticRegression(solver='lbfgs', penalty='l2'))\n",
    "    oneVsRest.fit(train_sets[x].values, train_set_labels[x].values.ravel())\n",
    "    \n",
    "    fold_results_ovr.loc[foldnum, 'Accuracy'] = oneVsOne.score(test_sets[x].values,\n",
    "                                                          test_set_labels[x].values.ravel())\n",
    "    \n",
    "    predictions = oneVsRest.predict_proba(test_sets[x].values)\n",
    "    score = ndcg_score(test_set_labels[x].as_matrix(), predictions, 5)\n",
    "    fold_results_ovr.loc[foldnum, 'ndcg']  = score\n",
    "    \n",
    "print (\"------OneVsRestClassifier accuracy and ndcg values------\")\n",
    "print (fold_results_ovr)\n",
    "print (\"------OneVsRestClassifier mean accuracy, ndcg values------\")\n",
    "print(fold_results_ovr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------OneVsRestClassifier accuracy and ndcg values------\n",
      "    Accuracy      ndcg\n",
      "1   0.608076  0.816163\n",
      "2   0.609885  0.816683\n",
      "3   0.604685  0.813442\n",
      "4   0.606840  0.815413\n",
      "5   0.606606  0.815870\n",
      "6   0.604872  0.814452\n",
      "7   0.607121  0.813901\n",
      "8   0.602670  0.813169\n",
      "9   0.610588  0.818077\n",
      "10  0.611525  0.816955\n",
      "------OneVsRestClassifier mean accuracy, ndcg values------\n",
      "Accuracy    0.607287\n",
      "ndcg        0.815412\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# liblinear and lbfgs perform  nearly the same...\n",
    "\n",
    "fold_results_ovr = pd.DataFrame()\n",
    "foldnum = 0\n",
    "for x in range(0, 10):\n",
    "    foldnum+=1\n",
    "    oneVsRest = OneVsRestClassifier(linear_model.LogisticRegression(solver='liblinear', penalty='l2'))\n",
    "    oneVsRest.fit(train_sets[x].values, train_set_labels[x].values.ravel())\n",
    "    \n",
    "    fold_results_ovr.loc[foldnum, 'Accuracy'] = oneVsOne.score(test_sets[x].values,\n",
    "                                                          test_set_labels[x].values.ravel())\n",
    "    \n",
    "    predictions = oneVsRest.predict_proba(test_sets[x].values)\n",
    "    score = ndcg_score(test_set_labels[x].as_matrix(), predictions, 5)\n",
    "    fold_results_ovr.loc[foldnum, 'ndcg']  = score\n",
    "    \n",
    "print (\"------OneVsRestClassifier accuracy and ndcg values------\")\n",
    "print (fold_results_ovr)\n",
    "print (\"------OneVsRestClassifier mean accuracy, ndcg values------\")\n",
    "print(fold_results_ovr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------OneVsRestClassifier accuracy and ndcg values------\n",
      "    Accuracy      ndcg\n",
      "1   0.608076  0.816199\n",
      "2   0.609885  0.816708\n",
      "3   0.604685  0.813442\n",
      "4   0.606840  0.815416\n",
      "5   0.606606  0.815858\n",
      "6   0.604872  0.814483\n",
      "7   0.607121  0.813844\n",
      "8   0.602670  0.813170\n",
      "9   0.610588  0.818057\n",
      "10  0.611525  0.816993\n",
      "------OneVsRestClassifier mean accuracy, ndcg values------\n",
      "Accuracy    0.607287\n",
      "ndcg        0.815417\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# liblinear does better with l1 penalty...\n",
    "\n",
    "fold_results_ovr = pd.DataFrame()\n",
    "foldnum = 0\n",
    "for x in range(0, 10):\n",
    "    foldnum+=1\n",
    "    oneVsRest = OneVsRestClassifier(linear_model.LogisticRegression(solver='liblinear', penalty='l1'))\n",
    "    oneVsRest.fit(train_sets[x].values, train_set_labels[x].values.ravel())\n",
    "    \n",
    "    fold_results_ovr.loc[foldnum, 'Accuracy'] = oneVsOne.score(test_sets[x].values,\n",
    "                                                          test_set_labels[x].values.ravel())\n",
    "    \n",
    "    predictions = oneVsRest.predict_proba(test_sets[x].values)\n",
    "    score = ndcg_score(test_set_labels[x].as_matrix(), predictions, 5)\n",
    "    fold_results_ovr.loc[foldnum, 'ndcg']  = score\n",
    "    \n",
    "print (\"------OneVsRestClassifier accuracy and ndcg values------\")\n",
    "print (fold_results_ovr)\n",
    "print (\"------OneVsRestClassifier mean accuracy, ndcg values------\")\n",
    "print(fold_results_ovr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------OneVsOneClassifier results for accuracy-----------\n",
      "    Accuracy_newton-cg  Accuracy_lbfgs  Accuracy_liblinear\n",
      "1             0.604770        0.604770            0.604770\n",
      "2             0.609218        0.609218            0.609218\n",
      "3             0.608556        0.608556            0.608556\n",
      "4             0.605622        0.605622            0.605622\n",
      "5             0.612152        0.612152            0.612152\n",
      "6             0.605338        0.605338            0.605338\n",
      "7             0.602499        0.602499            0.602499\n",
      "8             0.610969        0.611016            0.610969\n",
      "9             0.608840        0.608840            0.608840\n",
      "10            0.605858        0.605858            0.605858\n",
      "--------------OneVsOneClassifier mean accuracy----------------\n",
      "Accuracy_newton-cg    0.607382\n",
      "Accuracy_lbfgs        0.607387\n",
      "Accuracy_liblinear    0.607382\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanjana\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:285: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\sanjana\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:193: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression : One Versus One\n",
    "# Use validation set to find which solver to use \n",
    "\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "fold_results_ovo = pd.DataFrame()\n",
    "test_sets = []\n",
    "test_set_labels = []\n",
    "train_sets = []\n",
    "train_set_labels = []\n",
    "\n",
    "def ten_fold_oneVsOne(data, labels):\n",
    "    foldnum = 0\n",
    "    solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "    \n",
    "    for train, test in cross_validation.KFold(len(data), n_folds=10, shuffle=True):\n",
    "        [tr_data, te_data,\n",
    "         tr_target, te_target] = folds_to_split(data, labels,train,test)\n",
    "        \n",
    "        train_sets.append(tr_data)\n",
    "        train_set_labels.append(tr_target)\n",
    "        test_sets.append(te_data)\n",
    "        test_set_labels.append(te_target)\n",
    "    \n",
    "    foldnum = 0\n",
    "    for x in range(0, 10):\n",
    "        foldnum+=1\n",
    "        [bnb_train, bnb_validation, bnb_train_labels, bnb_validation_labels] = cross_validation.train_test_split(\n",
    "        train_sets[x], \n",
    "        train_set_labels[x], \n",
    "        test_size=0.11, \n",
    "        random_state=20160121)\n",
    "        for sol in solvers:\n",
    "            oneVsOne = OneVsOneClassifier(linear_model.LogisticRegression(solver=sol))\n",
    "            oneVsOne.fit(bnb_train.values, bnb_train_labels[bnb_train_labels.columns.values[0]].values)\n",
    "        \n",
    "            columnname = \"Accuracy\" + \"_\" + sol\n",
    "            fold_results_ovo.loc[foldnum, columnname] = oneVsOne.score(bnb_validation, bnb_validation_labels)\n",
    "    \n",
    "    #Now let's look at the results:\n",
    "    print (\"-----------OneVsOneClassifier results for accuracy-----------\")\n",
    "    print (fold_results_ovo)\n",
    "    \n",
    "    print (\"--------------OneVsOneClassifier mean accuracy----------------\")\n",
    "    print(fold_results_ovo.mean())\n",
    "    \n",
    "ten_fold_oneVsOne(train_users, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------OneVsOneClassifier results for accuracy-----------\n",
      "    Accuracy\n",
      "1   0.602080\n",
      "2   0.607309\n",
      "3   0.608855\n",
      "4   0.607871\n",
      "5   0.605247\n",
      "6   0.600609\n",
      "7   0.608199\n",
      "8   0.612087\n",
      "9   0.609932\n",
      "10  0.610354\n",
      "--------------OneVsOneClassifier mean accuracy----------------\n",
      "Accuracy    0.607254\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# lbfgs does best every time...\n",
    "\n",
    "fold_results_ovo = pd.DataFrame()\n",
    "foldnum = 0\n",
    "for x in range(0, 10):\n",
    "    \n",
    "    foldnum+=1\n",
    "    oneVsOne = OneVsOneClassifier(linear_model.LogisticRegression(solver='lbfgs'))\n",
    "    oneVsOne.fit(train_sets[x].values, train_set_labels[x].values.ravel())\n",
    "    \n",
    "    fold_results_ovo.loc[foldnum, 'Accuracy'] = oneVsOne.score(test_sets[x].values,\n",
    "                                                          test_set_labels[x].values.ravel())\n",
    "    \n",
    "\n",
    "print (\"-----------OneVsOneClassifier results for accuracy-----------\")\n",
    "print (fold_results_ovo)\n",
    "print (\"--------------OneVsOneClassifier mean accuracy----------------\")\n",
    "print(fold_results_ovo.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806944035724: solver newton-cg, estimators 100\n",
      "0.806926570731: solver newton-cg, estimators 200\n",
      "0.806926570731: solver newton-cg, estimators 250\n",
      "0.806926570731: solver lbfgs, estimators 100\n",
      "0.806909105737: solver lbfgs, estimators 200\n",
      "0.806926570731: solver lbfgs, estimators 250\n",
      "0.8052099633: solver newton-cg, estimators 100\n",
      "0.8052099633: solver newton-cg, estimators 200\n",
      "0.8052099633: solver newton-cg, estimators 250\n",
      "0.8052099633: solver lbfgs, estimators 100\n",
      "0.805244893287: solver lbfgs, estimators 200\n",
      "0.805244893287: solver lbfgs, estimators 250\n",
      "0.803373399797: solver newton-cg, estimators 100\n",
      "0.80339086479: solver newton-cg, estimators 200\n",
      "0.80339086479: solver newton-cg, estimators 250\n",
      "0.80339086479: solver lbfgs, estimators 100\n",
      "0.803408329784: solver lbfgs, estimators 200\n",
      "0.803408329784: solver lbfgs, estimators 250\n",
      "0.804220364724: solver newton-cg, estimators 100\n",
      "0.804237829718: solver newton-cg, estimators 200\n",
      "0.804237829718: solver newton-cg, estimators 250\n",
      "0.804150504749: solver lbfgs, estimators 100\n",
      "0.80420289973: solver lbfgs, estimators 200\n",
      "0.80420289973: solver lbfgs, estimators 250\n",
      "0.806519123405: solver newton-cg, estimators 100\n",
      "0.806519123405: solver newton-cg, estimators 200\n",
      "0.806519123405: solver newton-cg, estimators 250\n",
      "0.806536588399: solver lbfgs, estimators 100\n",
      "0.806536588399: solver lbfgs, estimators 200\n",
      "0.806554053393: solver lbfgs, estimators 250\n",
      "0.809865522653: solver newton-cg, estimators 100\n",
      "0.809865522653: solver newton-cg, estimators 200\n",
      "0.809882987647: solver newton-cg, estimators 250\n",
      "0.809848057659: solver lbfgs, estimators 100\n",
      "0.809865522653: solver lbfgs, estimators 200\n",
      "0.809865522653: solver lbfgs, estimators 250\n",
      "0.809451482845: solver newton-cg, estimators 100\n",
      "0.809468947838: solver newton-cg, estimators 200\n",
      "0.809468947838: solver newton-cg, estimators 250\n",
      "0.809329227889: solver lbfgs, estimators 100\n",
      "0.809294297901: solver lbfgs, estimators 200\n",
      "0.809311762895: solver lbfgs, estimators 250\n",
      "0.808491978033: solver newton-cg, estimators 100\n",
      "0.808509443026: solver newton-cg, estimators 200\n",
      "0.808509443026: solver newton-cg, estimators 250\n",
      "0.80838718807: solver lbfgs, estimators 100\n",
      "0.808439583052: solver lbfgs, estimators 200\n",
      "0.808439583052: solver lbfgs, estimators 250\n",
      "0.808080791909: solver newton-cg, estimators 100\n",
      "0.808080791909: solver newton-cg, estimators 200\n",
      "0.808080791909: solver newton-cg, estimators 250\n",
      "0.808220511858: solver lbfgs, estimators 100\n",
      "0.808220511858: solver lbfgs, estimators 200\n",
      "0.808220511858: solver lbfgs, estimators 250\n",
      "0.807305636316: solver newton-cg, estimators 100\n",
      "0.807358031297: solver newton-cg, estimators 200\n",
      "0.807358031297: solver newton-cg, estimators 250\n",
      "0.807218311347: solver lbfgs, estimators 100\n",
      "0.807235776341: solver lbfgs, estimators 200\n",
      "0.807235776341: solver lbfgs, estimators 250\n",
      "------Ada accuracy and ndcg values------\n",
      "    Accuracy_newton-cg_100  ndcg_newton-cg_100  Accuracy_newton-cg_200  \\\n",
      "1                 0.604013            0.806944                0.604297   \n",
      "2                 0.594974            0.805210                0.599849   \n",
      "3                 0.598240            0.803373                0.599896   \n",
      "4                 0.600274            0.804220                0.601599   \n",
      "5                 0.598949            0.806519                0.602309   \n",
      "6                 0.608982            0.809866                0.608934   \n",
      "7                 0.599754            0.809451                0.600984   \n",
      "8                 0.603350            0.808492                0.603634   \n",
      "9                 0.599091            0.808081                0.599896   \n",
      "10                0.602120            0.807306                0.603256   \n",
      "\n",
      "    ndcg_newton-cg_200  Accuracy_newton-cg_250  ndcg_newton-cg_250  \\\n",
      "1             0.806927                0.604581            0.806927   \n",
      "2             0.805210                0.600274            0.805210   \n",
      "3             0.803391                0.600606            0.803391   \n",
      "4             0.804238                0.601694            0.804238   \n",
      "5             0.806519                0.603019            0.806519   \n",
      "6             0.809866                0.609124            0.809883   \n",
      "7             0.809469                0.599991            0.809469   \n",
      "8             0.808509                0.604533            0.808509   \n",
      "9             0.808081                0.601079            0.808081   \n",
      "10            0.807358                0.602499            0.807358   \n",
      "\n",
      "    Accuracy_lbfgs_100  ndcg_lbfgs_100  Accuracy_lbfgs_200  ndcg_lbfgs_200  \\\n",
      "1             0.601647        0.806927            0.601647        0.806909   \n",
      "2             0.599044        0.805210            0.600322        0.805245   \n",
      "3             0.601931        0.803391            0.601789        0.803408   \n",
      "4             0.598997        0.804151            0.600133        0.804203   \n",
      "5             0.600416        0.806537            0.602404        0.806537   \n",
      "6             0.608650        0.809848            0.609360        0.809866   \n",
      "7             0.605007        0.809329            0.606048        0.809294   \n",
      "8             0.600180        0.808387            0.602593        0.808440   \n",
      "9             0.606048        0.808221            0.605480        0.808221   \n",
      "10            0.606379        0.807218            0.607420        0.807236   \n",
      "\n",
      "    Accuracy_lbfgs_250  ndcg_lbfgs_250  \n",
      "1             0.602451        0.806927  \n",
      "2             0.600085        0.805245  \n",
      "3             0.602025        0.803408  \n",
      "4             0.600416        0.804203  \n",
      "5             0.602593        0.806554  \n",
      "6             0.609124        0.809866  \n",
      "7             0.605906        0.809312  \n",
      "8             0.602167        0.808440  \n",
      "9             0.605291        0.808221  \n",
      "10            0.607089        0.807236  \n",
      "------Ada mean accuracy and ndcg values------\n",
      "Accuracy_newton-cg_100    0.600975\n",
      "ndcg_newton-cg_100        0.806946\n",
      "Accuracy_newton-cg_200    0.602465\n",
      "ndcg_newton-cg_200        0.806957\n",
      "Accuracy_newton-cg_250    0.602740\n",
      "ndcg_newton-cg_250        0.806958\n",
      "Accuracy_lbfgs_100        0.602830\n",
      "ndcg_lbfgs_100            0.806922\n",
      "Accuracy_lbfgs_200        0.603719\n",
      "ndcg_lbfgs_200            0.806936\n",
      "Accuracy_lbfgs_250        0.603715\n",
      "ndcg_lbfgs_250            0.806941\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Use ADA boost with logistic regression\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model, cross_validation\n",
    "\n",
    "test_sets = []\n",
    "test_set_labels = []\n",
    "train_sets = []\n",
    "train_set_labels = []\n",
    "fold_results_ada = pd.DataFrame()\n",
    "def ten_fold_ada_logistic(data, labels):\n",
    "    foldnum = 0\n",
    "    fold_results_ovr_est = pd.DataFrame()\n",
    "    for train, test in cross_validation.KFold(len(data), n_folds=10, shuffle=True, random_state=20160202):\n",
    "        \n",
    "        [tr_data, te_data,\n",
    "         tr_target, te_target] = folds_to_split(data, labels,train,test)\n",
    "        train_sets.append(tr_data)\n",
    "        train_set_labels.append(tr_target)\n",
    "        test_sets.append(te_data)\n",
    "        test_set_labels.append(te_target)\n",
    "    \n",
    "    solvers = ['newton-cg', 'lbfgs']\n",
    "    estimators = [100, 200, 250]\n",
    "    foldnum = 0\n",
    "    for x in range(0, 10):\n",
    "        foldnum+=1\n",
    "        [bnb_train, bnb_validation, bnb_train_labels, bnb_validation_labels] = cross_validation.train_test_split(\n",
    "            train_sets[x], \n",
    "            train_set_labels[x], \n",
    "            test_size=0.11, \n",
    "            random_state=20160121)\n",
    "        for sol in solvers:\n",
    "            for est in estimators:\n",
    "                \n",
    "                [tr_data, te_data,\n",
    "                 tr_target, te_target] = folds_to_split(data, labels,train,test)\n",
    "                ada = AdaBoostClassifier(learning_rate=0.3, n_estimators=est, \n",
    "                                     base_estimator=linear_model.LogisticRegression(solver=sol), \n",
    "                                     algorithm='SAMME')  \n",
    "                #print(tr_target)\n",
    "                ada.fit(bnb_train.values, bnb_train_labels[bnb_train_labels.columns.values[0]].values)\n",
    "                #print(oneVsRest.estimators_)\n",
    "        \n",
    "                fold_results_ada.loc[foldnum, 'Accuracy_' + sol + '_' + str(est)] = ada.score(bnb_validation, \n",
    "                                                                                            bnb_validation_labels)\n",
    "        \n",
    "                predictions = ada.predict_proba(bnb_validation)\n",
    "                score = ndcg_score(bnb_validation_labels.as_matrix(), predictions, 5)\n",
    "                fold_results_ada.loc[foldnum, 'ndcg_' + sol + '_' + str(est)]  = score\n",
    "                print(str(score) + \":\" + \" solver \" + sol + \", estimators \" + str(est))\n",
    "            \n",
    "    \n",
    "    #Now let's look at the results:\n",
    "    print (\"------Ada accuracy and ndcg values------\")\n",
    "    print (fold_results_ada)\n",
    "    print (\"------Ada mean accuracy and ndcg values------\")\n",
    "    print(fold_results_ada.mean())\n",
    "    \n",
    "ten_fold_ada_logistic(train_users, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVM : Not completing, takes forever to run\n",
    "\n",
    "from sklearn import svm\n",
    "    \n",
    "def ten_fold_svm(data, labels):\n",
    "    foldnum = 0\n",
    "    fold_results_ovr_est = pd.DataFrame()\n",
    "    fold_results_ada = pd.DataFrame()\n",
    "    for train, test in cross_validation.KFold(len(data), n_folds=10, shuffle=True, random_state=20160202):\n",
    "        foldnum+=1\n",
    "        [tr_data, te_data,\n",
    "         tr_target, te_target] = folds_to_split(data, labels,train,test)\n",
    "        vec_mach = svm.SVC(kernel='rbf', random_state=20160202)\n",
    "        #print(tr_target)\n",
    "        vec_mach.fit(tr_data.values, tr_target[tr_target.columns.values[0]].values)\n",
    "        \n",
    "        fold_results_ada.loc[foldnum, 'Accuracy'] = vec_mach.score(te_data, te_target)\n",
    "        \n",
    "        predictions = vec_mach.predict_proba(te_data)\n",
    "        score = ndcg_score(te_target.as_matrix(), predictions, 5)\n",
    "        fold_results_ada.loc[foldnum, 'ndcg']  = score\n",
    "        print(score)\n",
    "            \n",
    "    \n",
    "    #Now let's look at the results:\n",
    "    print (\"------svm accuracy values------\")\n",
    "    print (fold_results_ovr)\n",
    "    print (\"------svm mean accuracy values------\")\n",
    "    print(fold_results_ovr.mean())\n",
    "    \n",
    "ten_fold_svm(train_users, labels_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The best performance with logistic regression works with liblinear with l1 penalty \n",
    " ndcg score is 81.5417"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
